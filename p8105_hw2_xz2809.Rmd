---
title: "P8105_hw2_xz2809"
author: "Coco Zou"
date: "9/29/2018"
output: html_document
---

```{r}
library(tidyverse)
library(readxl)
```

## Problem 1

Here is the **code chunk** to read the data
```{r}
NYCtransit_data <-  read_csv(file = "./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv")
```

Now we are trying to clean the data, including retaining line, station, name, station latitude / longitude, routes served, entry, vending, entrance type, and ADA compliance. Also, we converted the entry variable from charater to logical variable. Here is the **code chunk**:

```{r}
NYCtransit_data = janitor::clean_names(NYCtransit_data)
NYCtransit_data_retain = 
  select(NYCtransit_data,line:vending, ada, ada_notes) %>% 
  mutate(entry=recode(entry,'YES'=TRUE, 'NO'=FALSE)) 
```

This dataset contains line, station name, station latitude and longtitude, routes served, whether there is entry and exist and the type of the entry, and ADA compliance. I imported the data first and then cleaned the names into all lower cases. Then I selected the only variables required, at last I converted the "entry" variable into logical variable "TRUE" and "FALSE". The diminsion of the given data is `r dim(NYCtransit_data_retain)`. This is not a tidy dataset yet, because the "route1" to "route11" are apparently data, but they appreared as variables at the first row. Therefore, we need do some reformatting later. 

Now here is the **code chunk** to analyze the data:

```{r}
num_stations = 
  distinct(NYCtransit_data_retain, line, station_name) %>% 
  dim()

num_stations_ada = 
  filter(NYCtransit_data_retain, ada=='TRUE') %>% 
  dim()

a = filter(NYCtransit_data_retain,entry == 'TRUE')
length_1 = length(a$entry)
b = filter(NYCtransit_data_retain,entry == 'TRUE', vending=='NO')
length_2= length(b$entry)
proportion=length_2/length_1
```

There are in total 465 distinct stations there and 468 stations are ADA compliant. Approximately, 3.9% of the stations with entrances and exists but without vending allow entrance. 

Now we are trying to reformat the dataset:

```{r}
NYCtransit_data_retain %>% 
  gather(key = route_number, value = route_name, route1:route11) %>% 
  filter(route_name=='A') %>% 
  distinct(station_name) %>% 
  dim()

NYCtransit_data_retain %>% 
  gather(key = route_number, value = route_name, route1:route11) %>% 
  filter(route_name=='A',ada==TRUE) %>% 
  distinct(station_name) %>% 
  dim()
```
There are 56 distinct stations serving A train and of the stations that serve the A train, there are 16 that are ADA compliant.

##Problem 2

This problem is about analyzing the data of Mr.Trash Wheel. Here is the **code chunk** to read and clean the data for mr trash wheel:
```{r}
trashwheel_data <-  
  read_excel( "data/HealthyHarborWaterWheelTotals2017-9-26.xlsx",range = cell_cols("A:N"), ) %>% 
  janitor::clean_names() %>% 
  head(256) %>% 
  mutate(
    sports_balls = round(sports_balls),
    sports_balls = as.integer(sports_balls)
  )
```

Here is the **code chunk** to read and clean the precipitation data for 2016 and 2017. 
```{r}
precipitation_2016_data<-
  read_excel( "data/HealthyHarborWaterWheelTotals2017-9-26.xlsx",sheet = "2016 Precipitation", range = "A2:B15") %>% 
  janitor::clean_names() %>%
  mutate(
    year = 2016
  )

precipitation_2017_data<-
  read_excel( "data/HealthyHarborWaterWheelTotals2017-9-26.xlsx",sheet = "2017 Precipitation", range = "A2:B15") %>% 
  janitor::clean_names() %>%
  mutate(
    year = 2017
  )

combine_precipitation_data =
  left_join(precipitation_2016_data,precipitation_2017_data,by = "month") 

combine_precipitation_data$month = month.name[combine_precipitation_data$month]
```

For the data of Mr Trash Wheel, there are in total 215 observations. The key variables in this data set is the weight of garbage has been salvaged, in tons and the volume that Mr. Trash Wheel has been to. The median number of sports balls in a dumpster in 2016 is `r median(filter(trashwheel_data, year==2016)$sports_balls)`. For the data of precipitation of 2016 and 2017, there are in total 12 observations in 2016, however only 8 observations in 2016. The key variable, is the total weight of precipitation in the given month. The total precipitation in 2017 is `r sum(head(combine_precipitation_data,8)$total.y)`. 

##Problem 3

This problem analyzes the BRFSS data. Here is the **code chunk** that loads the data for problem 3:
```{r}
library(p8105.datasets)
data("brfss_smart2010")
brfss_smart2010=
  janitor::clean_names(brfss_smart2010) %>% 
  select(-topic) %>% 
  select(-class) %>% 
  select(-question) %>% 
  select(-sample_size) %>%
  select(-(confidence_limit_low:geo_location))
```

Now we create a new variable showing the proportion of responses that were “Excellent” or “Very Good”:
```{r}
brfss_smart2010 %>% 
  filter(response=='Excellent' | response=='Very good') %>% 
  dim()

dim(brfss_smart2010)
```

```{r}
brfss_smart2010 %>% 
  distinct(locationabbr) %>% 
  dim()
```
The proportion of responses that were “Excellent” or “Very Good" is `r 4260/134203`.
In America, there are 50 states and every state has been represented. The one more variable is "DC", which is not an acutual state but the capital of USA.

```{r}
MaxTable <- function(x){
     m <- unique(x)
     m[which.max(tabulate(match(x,m)))]
}
MaxTable(brfss_smart2010$locationabbr)
```


```{r}
brfss_smart2010$data_value=as.numeric(brfss_smart2010$data_value)
brfss_excellent_2002 = 
  filter(brfss_smart2010,response=='Excellent') %>% 
  filter(year==2002)
summary(brfss_excellent_2002$data_value)
ggplot(brfss_excellent_2002,aes(x=data_value))+geom_histogram()
```

The most most observed state is NJ. The median of the “Excellent” response value is 23.60. 




